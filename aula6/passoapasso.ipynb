{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instalação das bibliotecas (execute apenas se não estiverem instaladas)\n",
    "# !pip install gensim matplotlib scikit-learn pandas numpy spacy plotly\n",
    "\n",
    "# Importações básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download de recursos NLTK (se necessário)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto 1: Este filme é incrível, adorei a atuação do protagonista\n",
      "Texto 2: A direção de fotografia é espetacular e o roteiro é envolvente\n",
      "Texto 3: Péssimo filme, desperdicei meu tempo assistindo isso\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo - críticas de filmes (simplificadas)\n",
    "textos = [\n",
    "    \"Este filme é incrível, adorei a atuação do protagonista\",\n",
    "    \"A direção de fotografia é espetacular e o roteiro é envolvente\",\n",
    "    \"Péssimo filme, desperdicei meu tempo assistindo isso\",\n",
    "    \"Os atores são talentosos mas o roteiro é fraco\",\n",
    "    \"Cinematografia belíssima, recomendo assistir no cinema\",\n",
    "    \"Não gostei da história, personagens mal desenvolvidos\",\n",
    "    \"A trilha sonora combina perfeitamente com as cenas\",\n",
    "    \"Filme entediante, previsível do início ao fim\",\n",
    "    \"Os efeitos especiais são impressionantes, tecnologia de ponta\",\n",
    "    \"História emocionante, chorei no final do filme\"\n",
    "]\n",
    "\n",
    "# Verificando os dados\n",
    "for i, texto in enumerate(textos[:3]):  # Mostrando apenas os 3 primeiros\n",
    "    print(f\"Texto {i+1}: {texto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de texto original:\n",
      "Este filme é incrível, adorei a atuação do protagonista\n",
      "\n",
      "Depois do pré-processamento:\n",
      "['filme', 'incrível', 'adorei', 'atuação', 'protagonista']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remover caracteres especiais e números\n",
    "    texto = re.sub(r'[^a-záàâãéèêíïóôõöúçñ ]', '', texto)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(texto)\n",
    "\n",
    "    # Remover stopwords (opcional, dependendo da aplicação)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Aplicar pré-processamento a todos os textos\n",
    "textos_preprocessados = [preprocessar_texto(texto) for texto in textos]\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"Exemplo de texto original:\")\n",
    "print(textos[0])\n",
    "print(\"\\nDepois do pré-processamento:\")\n",
    "print(textos_preprocessados[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=100,  # Dimensão dos vetores\n",
    "    window=5,  # Tamanho da janela\n",
    "    min_count=1,  # Ignorar palavras com contagem menor que este valor\n",
    "    workers=4,  # Número de threads\n",
    "    sg=0  # 0 para CBOW, 1 para Skip-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.key_to_index)  # Verificar o vocabulário\n",
    "# Exibir o vetor de uma palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vetor da palavra 'assistindo':\n",
      "[-0.00515709 -0.00666967 -0.00777227  0.00830602 -0.00198317 -0.00685752\n",
      " -0.00414707  0.005152   -0.00286454 -0.00375924  0.00162542 -0.00278873\n",
      " -0.00158943  0.00107075 -0.00298073  0.00851917  0.00391437 -0.00995964\n",
      "  0.0062646  -0.00675831  0.00076979  0.00440522 -0.00509948 -0.00210761\n",
      "  0.00809539 -0.00423876 -0.00763924  0.00925958 -0.00215644 -0.00471729\n",
      "  0.00856475  0.00428233  0.00433249  0.00927869 -0.00845984  0.00525443\n",
      "  0.00204342  0.00419017  0.0016977   0.00446892  0.0044879   0.00610736\n",
      " -0.0032045  -0.00457429 -0.00042552  0.00253149 -0.00326126  0.00606048\n",
      "  0.00415368  0.00777164  0.00257221  0.00811684 -0.00138476  0.00807491\n",
      "  0.00371549 -0.00805182 -0.00393021 -0.00247027  0.00489987 -0.00086822\n",
      " -0.00283821  0.00782966  0.00932731 -0.001615   -0.00516088 -0.0046998\n",
      " -0.00484762 -0.00960214  0.00136941 -0.0042258   0.00253196  0.00562077\n",
      " -0.00405985 -0.00959664  0.00155138 -0.00669963  0.00249996 -0.00378492\n",
      "  0.00707881  0.00064249  0.0035558  -0.00274    -0.00170708  0.0076506\n",
      "  0.0014075  -0.00585364 -0.00783109  0.00123036  0.00645413  0.00555628\n",
      " -0.00897213  0.00859512  0.00404991  0.00746979  0.00975531 -0.0072884\n",
      " -0.00903593  0.00583742  0.00939372  0.00350988]\n"
     ]
    }
   ],
   "source": [
    "if \"assistindo\" in model.wv.key_to_index:\n",
    "    print(\"\\nVetor da palavra 'assistindo':\")\n",
    "    print(model.wv['assistindo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palavras mais similares a 'filme':\n",
      "direção: 0.2189\n",
      "assistindo: 0.2164\n",
      "impressionantes: 0.1956\n",
      "cinema: 0.1694\n",
      "efeitos: 0.1517\n"
     ]
    }
   ],
   "source": [
    "# Encontrar palavras mais similares a 'filme'\n",
    "if 'filme' in model.wv:\n",
    "    similares = model.wv.most_similar('filme', topn=5)\n",
    "    print(\"\\nPalavras mais similares a 'filme':\")\n",
    "    for palavra, similaridade in similares:\n",
    "        print(f\"{palavra}: {similaridade:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"vasco.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7bec9594c8c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vascao = Word2Vec.load(\"vasco.model\")\n",
    "vascao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50']\n"
     ]
    }
   ],
   "source": [
    "print([nome for nome in list(api.info()['models'].keys())[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_pretreino = api.load(\"glove-wiki-gigaword-100\")  # Carregar o modelo GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analogia: rei - homem + mulher =\n",
      "queen: 0.7699\n",
      "monarch: 0.6843\n",
      "throne: 0.6756\n"
     ]
    }
   ],
   "source": [
    "# Usando modelo pré-treinado para analogias (se disponível)\n",
    "try:\n",
    "    # Famosa analogia: rei - homem + mulher = rainha\n",
    "    if all(word in modelo_pretreino for word in ['king', 'man', 'woman']):\n",
    "        resultado = modelo_pretreino.most_similar(\n",
    "            positive=['king', 'woman'],\n",
    "            negative=['man'],\n",
    "            topn=3\n",
    "        )\n",
    "        print(\"\\nAnalogia: rei - homem + mulher =\")\n",
    "        for palavra, similaridade in resultado:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except:\n",
    "    print(\"Não foi possível realizar a operação de analogia com o modelo disponível\")\n",
    "\n",
    "# Podemos tentar outras analogias com nosso modelo treinado\n",
    "# Por exemplo: bom - positivo + negativo = ruim\n",
    "# (Dependendo do tamanho do corpus, pode não funcionar bem para modelos pequenos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similaridade entre pares de palavras:\n",
      "filme - cinema: 0.1694173514842987\n",
      "bom - ruim: Uma ou ambas as palavras não estão no vocabulário\n",
      "ator - atuação: Uma ou ambas as palavras não estão no vocabulário\n",
      "filme - protagonista: -0.11389946192502975\n"
     ]
    }
   ],
   "source": [
    "# Função para calcular similaridade entre pares de palavras\n",
    "def calcular_similaridade(modelo, pares_palavras):\n",
    "    resultados = []\n",
    "    for par in pares_palavras:\n",
    "        palavra1, palavra2 = par\n",
    "        if palavra1 in modelo.wv and palavra2 in modelo.wv:\n",
    "            similaridade = modelo.wv.similarity(palavra1, palavra2)\n",
    "            resultados.append((par, similaridade))\n",
    "        else:\n",
    "            resultados.append((par, \"Uma ou ambas as palavras não estão no vocabulário\"))\n",
    "    return resultados\n",
    "\n",
    "# Pares de palavras para testar\n",
    "pares = [\n",
    "    ('filme', 'cinema'),\n",
    "    ('bom', 'ruim'),\n",
    "    ('ator', 'atuação'),\n",
    "    ('filme', 'protagonista')\n",
    "]\n",
    "\n",
    "# Calcular similaridades\n",
    "similaridades = calcular_similaridade(model, pares)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nSimilaridade entre pares de palavras:\")\n",
    "for (palavra1, palavra2), similaridade in similaridades:\n",
    "    if isinstance(similaridade, float):\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade:.4f}\")\n",
    "    else:\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados com TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "\n",
      "Resultados com Word Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dados rotulados para exemplo\n",
    "textos_rotulados = textos  # Usando os mesmos textos de antes\n",
    "sentimentos = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1: positivo, 0: negativo\n",
    "\n",
    "# Função para gerar vetores de documento usando embeddings\n",
    "def texto_para_vetor(texto, modelo):\n",
    "    \"\"\"Converte um texto em um vetor médio dos embeddings de suas palavras\"\"\"\n",
    "    palavras = preprocessar_texto(texto)\n",
    "    # Filtrar palavras que estão no vocabulário do modelo\n",
    "    palavras_no_vocab = [p for p in palavras if p in modelo.wv]\n",
    "    if not palavras_no_vocab:\n",
    "        # Se nenhuma palavra estiver no vocabulário, retorna vetor de zeros\n",
    "        return np.zeros(modelo.vector_size)\n",
    "    # Calcular a média dos vetores das palavras\n",
    "    vetores = [modelo.wv[palavra] for palavra in palavras_no_vocab]\n",
    "    return np.mean(vetores, axis=0)\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    textos_rotulados, sentimentos, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Abordagem com TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "clf_tfidf = LogisticRegression(random_state=42)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# 2. Abordagem com Word Embeddings\n",
    "X_train_emb = np.array([texto_para_vetor(texto, model) for texto in X_train])\n",
    "X_test_emb = np.array([texto_para_vetor(texto, model) for texto in X_test])\n",
    "\n",
    "clf_emb = LogisticRegression(random_state=42)\n",
    "clf_emb.fit(X_train_emb, y_train)\n",
    "y_pred_emb = clf_emb.predict(X_test_emb)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\nResultados com TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "print(\"\\nResultados com Word Embeddings:\")\n",
    "print(classification_report(y_test, y_pred_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto: O banco está cheio de\n",
      "Vetor (10 primeiras dimensões): [-2.761275    2.7341504   1.9384544   0.34338498  1.0899546  -4.300789\n",
      "  1.5131965   3.7354984   1.1549678  -1.9115787 ]\n",
      "Dimensão do vetor: 96\n",
      "--------------------------------------------------\n",
      "Contexto: Eu sentei no banco da praça.\n",
      "Vetor (10 primeiras dimensões): [-2.8606548   3.2033174   4.2152634   2.3896182   4.4038973  -3.509808\n",
      "  4.1706643   5.813535   -0.12308097 -0.5815916 ]\n",
      "Dimensão do vetor: 96\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar modelo spaCy (pequeno)\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")  # Para português\n",
    "    # Alternativa para inglês: nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Exemplo de texto\n",
    "    texto = \"O banco está cheio de dinheiro. Eu sentei no banco da praça.\"\n",
    "\n",
    "    # Processar o texto\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # Examinar embeddings para cada ocorrência da palavra \"banco\"\n",
    "    for token in doc:\n",
    "        if token.text.lower() == \"banco\":\n",
    "            contexto = doc[max(0, token.i-3):min(len(doc), token.i+4)]\n",
    "            print(f\"Contexto: {contexto}\")\n",
    "            print(f\"Vetor (10 primeiras dimensões): {token.vector[:10]}\")\n",
    "            print(f\"Dimensão do vetor: {len(token.vector)}\")\n",
    "            print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar spaCy: {e}\")\n",
    "    print(\"Talvez seja necessário instalar os modelos com:\")\n",
    "    print(\"python -m spacy download pt_core_news_sm\")\n",
    "    print(\"ou\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
